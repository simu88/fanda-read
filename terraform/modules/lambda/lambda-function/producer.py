import os
import json
import logging
import boto3
from kafka import KafkaProducer
from kafka.errors import KafkaError
# === 1. AbstractTokenProviderë¥¼ import í•©ë‹ˆë‹¤. ===
from kafka.sasl.oauth import AbstractTokenProvider
from aws_msk_iam_sasl_signer import MSKAuthTokenProvider
from datetime import datetime 

# --- ë¡œê¹… ì„¤ì • ---
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# --- ìƒìˆ˜ ë° í™˜ê²½ ë³€ìˆ˜ ---
REGION = os.environ.get('AWS_REGION', 'us-east-1')
CLUSTER_ARN = os.environ.get('MSK_CLUSTER_ARN')
TOPIC = os.environ.get('MSK_TOPIC', 'fanda-notifications')

# === 2. AbstractTokenProviderë¥¼ ìƒì†ë°›ë„ë¡ í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤. ===
class MSKTokenProvider(AbstractTokenProvider):
    def __init__(self, region: str):
        self.region = region

    def token(self) -> str:
        """
        kafka-python ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¸ì¦ í† í°ì„ í•„ìš”ë¡œ í•  ë•Œë§ˆë‹¤ ì´ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        ìë™ìœ¼ë¡œ ìƒˆ í† í°ì„ ìƒì„±í•˜ì—¬ ë°˜í™˜í•˜ë¯€ë¡œ í† í° ë§Œë£Œë¥¼ ê±±ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
        """
        token, _ = MSKAuthTokenProvider.generate_auth_token(self.region)
        logger.info("Successfully generated new MSK IAM Auth Token for Producer.")
        return token

# --- Kafka Producer ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ---
producer = None

def get_kafka_producer():
    """
    ì „ì—­ producer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ì‹±ê¸€í†¤ íŒ¨í„´).
    Lambdaê°€ ì¬ì‚¬ìš©ë  ë•Œë§ˆë‹¤ ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
    """
    global producer
    if producer:
        logger.info("Reusing existing Kafka producer instance.")
        return producer

    if not CLUSTER_ARN:
        raise ValueError("MSK_CLUSTER_ARN environment variable is not set")

    logger.info("Initializing Kafka producer for the first time (cold start)...")
    try:
        # 1. ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¸Œë¡œì»¤ ë™ì  ì¡°íšŒ
        kafka_client = boto3.client('kafka', region_name=REGION)
        bootstrap_info = kafka_client.get_bootstrap_brokers(ClusterArn=CLUSTER_ARN)
        bootstrap_servers = bootstrap_info['BootstrapBrokerStringSaslIam']
        logger.info(f"Dynamically fetched bootstrap servers: {bootstrap_servers}")

        # 2. ë™ì  í† í° ì œê³µì ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        token_provider = MSKTokenProvider(region=REGION)

        # 3. Kafka Producer ì´ˆê¸°í™” (ë³€ê²½ ì—†ìŒ)
        producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            security_protocol='SASL_SSL',
            sasl_mechanism='OAUTHBEARER',
            sasl_oauth_token_provider=token_provider,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            retries=5,
            request_timeout_ms=30000
        )
        logger.info("Kafka producer initialized successfully.")
        return producer
    except Exception as e:
        logger.error(f"Failed to create Kafka producer: {e}", exc_info=True)
        raise

# --- Lambda í•¸ë“¤ëŸ¬ í•¨ìˆ˜ ---
def lambda_handler(event, context):
    try:
        kafka_producer = get_kafka_producer()
    except Exception as e:
        return {'statusCode': 500, 'body': json.dumps(f'Failed to initialize Kafka Producer: {str(e)}')}

    records = event.get('Records', [])
    logger.info(f"Received {len(records)} S3 records to process.")

    for record in records:
        try:
            s3_info = record.get('s3', {})
            bucket = s3_info.get('bucket', {}).get('name')
            key = s3_info.get('object', {}).get('key')
            if not bucket or not key:
                logger.warning(f"Skipping record due to missing bucket or key: {record}")
                continue

            # os.path.basenameì„ ì‚¬ìš©í•´ ì „ì²´ ê²½ë¡œ(key)ì—ì„œ íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            clean_filename = os.path.basename(key)  
            upload_time = record.get('eventTime')
            version_from_upload = "N/A" # ê¸°ë³¸ê°’
            if upload_time:
                try:
                    dt_object = datetime.fromisoformat(upload_time.replace('Z', '+00:00'))
                    version_from_upload = dt_object.strftime('%y.%m.%d')
                except (ValueError, TypeError):
                    version_from_upload = upload_time.split('T')[0]

            #  category ê²°ì • ë¡œì§
            category = "general" # ê¸°ë³¸ê°’
            if key.startswith("reports/positive/"):
                category = "positive"
            elif key.startswith("reports/negative/"):
                category = "negative"
            elif key.startswith("reports/feedback/"):
                category = "feedback"


            message = {
                'fileName': clean_filename,
                'bucketName': bucket,
                'fileSize': s3_info.get('object', {}).get('size'),
                'uploadTime': record.get('eventTime'),
                'version': version_from_upload,
                's3Url': f"s3://{bucket}/{key}",
                "category": category   # ğŸ‘ˆ ì»¨ìŠˆë¨¸ê°€ ì´ ê°’ì„ í™œìš©í•©ë‹ˆë‹¤
            }

            

            future = kafka_producer.send(TOPIC, message)
            result = future.get(timeout=10)
            logger.info(f"Message sent to topic={result.topic} partition={result.partition} offset={result.offset}")

        except KafkaError as e:
            logger.error(f"Failed to send message to Kafka for object {key}: {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred while processing record for object {key}: {e}", exc_info=True)

    kafka_producer.flush()

    return {
        'statusCode': 200,
        'body': json.dumps('Finished processing S3 events successfully.')
    }
